<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="./_next/static/css/875eb13cb899d14e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="./_next/static/chunks/webpack-d7ecf9c8ecc55f45.js"/><script src="./_next/static/chunks/4bd1b696-9924fae48e609361.js" async=""></script><script src="./_next/static/chunks/517-dee78505a477fd66.js" async=""></script><script src="./_next/static/chunks/main-app-9c380134f3aa40b0.js" async=""></script><script src="./_next/static/chunks/944-9b7549ec5596ba95.js" async=""></script><script src="./_next/static/chunks/app/layout-5db32462581884c2.js" async=""></script><script src="./_next/static/chunks/392-ced4cbe0c5868303.js" async=""></script><script src="./_next/static/chunks/113-8d32fc47c3af7cd3.js" async=""></script><script src="./_next/static/chunks/app/datasets/page-c2705cce1ea496d1.js" async=""></script><link rel="preload" href="//unpkg.com/same-runtime/dist/index.global.js" as="script" crossorigin=""/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><title>Dr. Chen Chen</title><meta name="description" content="Generated by create next app"/><link href="https://fonts.googleapis.com/css2?family=Geist&amp;family=Geist+Mono&amp;display=swap" rel="stylesheet"/><script src="./_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased"><div class="antialiased"><div class="min-h-screen bg-gradient-to-br from-slate-50 to-blue-50"><nav class="sticky top-0 z-50 border-b bg-white/80 backdrop-blur-md"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between h-16"><div class="flex items-center"><a class="text-xl font-bold text-slate-900" href="/">Dr. Chen Chen</a></div><div class="hidden md:flex items-center space-x-8"><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/">Home</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/biography/">Biography</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/group/">Group</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/publications/">Publications</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/patents/">Patents</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/datasets/">Datasets</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/teaching/">Teaching</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/activity/">Activities</a><a class="text-slate-600 hover:text-slate-900 transition-colors" href="/join/">Join Us</a></div><div class="md:hidden"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:bg-accent hover:text-accent-foreground h-8 rounded-md px-3 text-xs" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R3bttb:" data-state="closed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu w-5 h-5"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button></div></div></div></nav><main class="container mx-auto px-4 py-12"><div class="max-w-5xl mx-auto"><div class="text-center mb-12"><h1 class="text-4xl font-bold text-gray-900 mb-4">Datasets</h1><p class="text-lg text-gray-600 max-w-3xl mx-auto">Research datasets for computer vision, human action recognition, and geo-localization. All data is only for research purposes, unless stated differently. Please make sure to reference the authors properly when using the data.</p></div><div class="grid grid-cols-1 md:grid-cols-4 gap-6 mb-12"><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="p-6 text-center"><div class="text-3xl font-bold text-blue-600 mb-2">4</div><div class="text-sm text-gray-600">Total Datasets</div></div></div><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="p-6 text-center"><div class="text-3xl font-bold text-purple-600 mb-2">1</div><div class="text-sm text-gray-600">Video Datasets</div></div></div><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="p-6 text-center"><div class="text-3xl font-bold text-green-600 mb-2">2</div><div class="text-sm text-gray-600">Geo-localization</div></div></div><div class="rounded-xl border bg-card text-card-foreground shadow"><div class="p-6 text-center"><div class="text-3xl font-bold text-orange-600 mb-2">1</div><div class="text-sm text-gray-600">Multimodal</div></div></div></div><div class="space-y-8"><h2 class="text-2xl font-semibold text-gray-900 mb-6">Research Datasets</h2><div class="rounded-xl border bg-card text-card-foreground shadow mb-8 hover:shadow-lg transition-shadow duration-200"><div class="flex flex-col space-y-1.5 p-6"><div class="flex items-start justify-between gap-4"><div class="flex-1"><div class="tracking-tight text-xl font-bold text-gray-900 mb-2">Video Anomaly Detection Dataset</div><div class="flex items-center gap-2 mb-3"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent hover:bg-secondary/80 bg-purple-100 text-purple-800"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-video h-4 w-4"><path d="m16 13 5.223 3.482a.5.5 0 0 0 .777-.416V7.87a.5.5 0 0 0-.752-.432L16 10.5"></path><rect x="2" y="6" width="14" height="12" rx="2"></rect></svg><span class="ml-1 capitalize">video</span></div></div></div></div></div><div class="p-6 pt-0 space-y-4"><div class="w-full h-48 bg-gray-100 rounded-lg overflow-hidden"><img alt="Video Anomaly Detection Dataset" loading="lazy" width="400" height="300" decoding="async" data-nimg="1" class="w-full h-full object-cover" style="color:transparent" src="/api/placeholder/400/300"/></div><div class="space-y-3"><p class="text-gray-700 leading-relaxed">UCF-Crime dataset is a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism.</p><p class="text-gray-600 text-sm leading-relaxed">These anomalies are selected because they have a significant impact on public safety. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities.</p></div><div class="bg-gray-50 p-4 rounded-lg space-y-2"><h4 class="font-semibold text-gray-900">Video Anomaly Detection Dataset</h4><p class="text-sm text-gray-600">Waqas Sultani, Chen Chen, Mubarak Shah</p><p class="text-sm font-medium text-blue-700">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<!-- -->, <!-- -->2018</p></div><div class="flex flex-wrap gap-2"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text h-3 w-3 mr-1"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>Paper</button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-3 w-3 mr-1"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg>Code</button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link h-3 w-3 mr-1"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>Project</button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-play h-3 w-3 mr-1"><polygon points="6 3 20 12 6 21 6 3"></polygon></svg>Video</button></div><div class="bg-blue-50 p-4 rounded-lg space-y-2"><h5 class="font-medium text-blue-900 flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-download h-4 w-4"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path><polyline points="7 10 12 15 17 10"></polyline><line x1="12" x2="12" y1="15" y2="3"></line></svg>Download Options</h5><div class="space-y-1"><p class="text-sm font-medium text-blue-800">Primary Download:</p><p class="text-sm text-blue-700 font-mono bg-white px-2 py-1 rounded">www.crcv.ucf.edu/data1/chenchen/UCF_Crimes.zip</p></div><div class="space-y-1"><p class="text-sm font-medium text-blue-800">Alternative Download (Dropbox):</p><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3 text-blue-700"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-download h-3 w-3 mr-1"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path><polyline points="7 10 12 15 17 10"></polyline><line x1="12" x2="12" y1="15" y2="3"></line></svg>Multiple Files</button></div><div class="bg-amber-50 border border-amber-200 p-2 rounded text-sm text-amber-800"><strong>Note:</strong> <!-- -->The &quot;Anomaly_Train.txt&quot; file in the zip file is corrupted, please download it here: Anomaly_Train.txt</div></div></div></div><div class="rounded-xl border bg-card text-card-foreground shadow mb-8 hover:shadow-lg transition-shadow duration-200"><div class="flex flex-col space-y-1.5 p-6"><div class="flex items-start justify-between gap-4"><div class="flex-1"><div class="tracking-tight text-xl font-bold text-gray-900 mb-2">VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval</div><div class="flex items-center gap-2 mb-3"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent hover:bg-secondary/80 bg-green-100 text-green-800"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-map-pin h-4 w-4"><path d="M20 10c0 4.993-5.539 10.193-7.399 11.799a1 1 0 0 1-1.202 0C9.539 20.193 4 14.993 4 10a8 8 0 0 1 16 0"></path><circle cx="12" cy="10" r="3"></circle></svg><span class="ml-1 capitalize">geolocalization</span></div></div></div></div></div><div class="p-6 pt-0 space-y-4"><div class="w-full h-48 bg-gray-100 rounded-lg overflow-hidden"><img alt="VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval" loading="lazy" width="400" height="300" decoding="async" data-nimg="1" class="w-full h-full object-cover" style="color:transparent" src="/api/placeholder/400/300"/></div><div class="space-y-3"><p class="text-gray-700 leading-relaxed">Cross-view image geo-localization aims to determine the locations of street-view query images by matching with GPS-tagged reference images from aerial view. Recent works have achieved surprisingly high retrieval accuracy on city-scale datasets.</p><p class="text-gray-600 text-sm leading-relaxed">However, these results rely on the assumption that there exists a reference image exactly centered at the location of any query image, which is not applicable for practical scenarios. In this paper, we redefine this problem with a more realistic assumption that the query image can be arbitrary in the area of interest and the reference images are captured before the queries emerge. This assumption breaks the one-to-one retrieval setting of existing datasets as the queries and reference images are not perfectly aligned pairs, and there may be multiple reference images covering one query location. To bridge the gap between this realistic setting and existing datasets, we propose a new large-scale benchmark –VIGOR– for crossView Image Geo-localization beyond One-to-one Retrieval.</p></div><div class="bg-gray-50 p-4 rounded-lg space-y-2"><h4 class="font-semibold text-gray-900">VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval</h4><p class="text-sm text-gray-600">Sijie Zhu, Taojiannan Yang, Chen Chen</p><p class="text-sm font-medium text-blue-700">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<!-- -->, <!-- -->2021</p></div><div class="flex flex-wrap gap-2"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text h-3 w-3 mr-1"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>Paper</button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-3 w-3 mr-1"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg>Dataset &amp; Code</button></div></div></div><div class="rounded-xl border bg-card text-card-foreground shadow mb-8 hover:shadow-lg transition-shadow duration-200"><div class="flex flex-col space-y-1.5 p-6"><div class="flex items-start justify-between gap-4"><div class="flex-1"><div class="tracking-tight text-xl font-bold text-gray-900 mb-2">Cross-View Geolocalization Dataset</div><div class="flex items-center gap-2 mb-3"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent hover:bg-secondary/80 bg-green-100 text-green-800"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-map-pin h-4 w-4"><path d="M20 10c0 4.993-5.539 10.193-7.399 11.799a1 1 0 0 1-1.202 0C9.539 20.193 4 14.993 4 10a8 8 0 0 1 16 0"></path><circle cx="12" cy="10" r="3"></circle></svg><span class="ml-1 capitalize">geolocalization</span></div></div></div></div></div><div class="p-6 pt-0 space-y-4"><div class="w-full h-48 bg-gray-100 rounded-lg overflow-hidden"><img alt="Cross-View Geolocalization Dataset" loading="lazy" width="400" height="300" decoding="async" data-nimg="1" class="w-full h-full object-cover" style="color:transparent" src="/api/placeholder/400/300"/></div><div class="space-y-3"><p class="text-gray-700 leading-relaxed">UCF cross-view geolocalization dataset is created for the geo-localization task using cross-view image matching. The dataset has street view and bird&#x27;s eye view image pairs around downtown Pittsburgh, Orlando and part of Manhattan.</p><p class="text-gray-600 text-sm leading-relaxed">There are 1,586, 1,324 and 5,941 GPS locations in Pittsburgh, Orlando and Manhattan, respectively. We utilize DualMaps to generate side-by-side street view and bird&#x27;s eye view images at each GPS location with the same heading direction. The street view images are from Google and the overhead 45 degree bird&#x27;s eye view images are from Bing. For each GPS location, four image pairs are generated with camera heading directions of 0 degree, 90 degree, 180 degree and 270 degree. In order to learn the deep network for building matching, we annotate corresponding buildings in every street view and bird&#x27;s eye view image pair.</p></div><div class="bg-gray-50 p-4 rounded-lg space-y-2"><h4 class="font-semibold text-gray-900">Cross-View Geolocalization Dataset</h4><p class="text-sm text-gray-600">Yicong Tian, Chen Chen, Mubarak Shah</p><p class="text-sm font-medium text-blue-700">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<!-- -->, <!-- -->2017</p></div><div class="flex flex-wrap gap-2"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text h-3 w-3 mr-1"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>Paper</button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link h-3 w-3 mr-1"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>Project</button></div></div></div><div class="rounded-xl border bg-card text-card-foreground shadow mb-8 hover:shadow-lg transition-shadow duration-200"><div class="flex flex-col space-y-1.5 p-6"><div class="flex items-start justify-between gap-4"><div class="flex-1"><div class="tracking-tight text-xl font-bold text-gray-900 mb-2">UTD-MHAD Dataset</div><div class="flex items-center gap-2 mb-3"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent hover:bg-secondary/80 bg-orange-100 text-orange-800"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-activity h-4 w-4"><path d="M22 12h-2.48a2 2 0 0 0-1.93 1.46l-2.35 8.36a.25.25 0 0 1-.48 0L9.24 2.18a.25.25 0 0 0-.48 0l-2.35 8.36A2 2 0 0 1 4.49 12H2"></path></svg><span class="ml-1 capitalize">multimodal</span></div></div></div></div></div><div class="p-6 pt-0 space-y-4"><div class="w-full h-48 bg-gray-100 rounded-lg overflow-hidden"><img alt="UTD-MHAD Dataset" loading="lazy" width="400" height="300" decoding="async" data-nimg="1" class="w-full h-full object-cover" style="color:transparent" src="/api/placeholder/400/300"/></div><div class="space-y-3"><p class="text-gray-700 leading-relaxed">UTD-MHAD dataset was collected as part of our research on human action recognition using fusion of depth and inertial sensor data. The objective of this research has been to develop algorithms for more robust human action recognition using fusion of data from differing modality sensors.</p><p class="text-gray-600 text-sm leading-relaxed">The UTD-MHAD dataset consists of 27 different actions: (1) right arm swipe to the left, (2) right arm swipe to the right, (3) right hand wave, (4) two hand front clap, (5) right arm throw, (6) cross arms in the chest, (7) basketball shoot, (8) right hand draw x, (9) right hand draw circle (clockwise), (10) right hand draw circle (counter clockwise), (11) draw triangle, (12) bowling (right hand), (13) front boxing, (14) baseball swing from right, (15) tennis right hand forehand swing, (16) arm curl (two arms), (17) tennis serve, (18) two hand push, (19) right hand knock on door, (20) right hand catch an object, (21) right hand pick up and throw, (22) jogging in place, (23) walking in place, (24) sit to stand, (25) stand to sit, (26) forward lunge (left foot forward), (27) squat (two arms stretch out).</p></div><div class="bg-gray-50 p-4 rounded-lg space-y-2"><h4 class="font-semibold text-gray-900">UTD-MHAD Dataset</h4><p class="text-sm text-gray-600">Chen Chen, Roozbeh Jafari, Nasser Kehtarnavaz</p><p class="text-sm font-medium text-blue-700">IEEE International Conference on Image Processing (ICIP)<!-- -->, <!-- -->2015</p></div><div class="flex flex-wrap gap-2"><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text h-3 w-3 mr-1"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg>Paper</button><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground rounded-md text-xs h-8 px-3"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link h-3 w-3 mr-1"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg>Dataset Website</button></div></div></div></div><div class="mt-12 bg-amber-50 border border-amber-200 rounded-lg p-6"><h3 class="text-lg font-semibold text-amber-900 mb-2">Important Notice</h3><p class="text-amber-800">All datasets provided here are intended for research purposes only. Please ensure proper citation of the original authors and papers when using these datasets in your research. Commercial use may require separate licensing agreements.</p></div></div></main><footer class="bg-slate-900 text-white py-12 mt-16"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="text-center"><h3 class="text-lg font-semibold mb-4">Dr. Chen Chen</h3><p class="text-slate-400 mb-4">Associate Professor, Center for Research in Computer Vision<br/>University of Central Florida</p><div class="flex justify-center space-x-6"><a target="_blank" class="text-slate-400 hover:text-white transition-colors" href="https://scholar.google.com/citations?hl=en&amp;user=TuEwcZ0AAAAJ"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link w-5 h-5"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg></a><a target="_blank" class="text-slate-400 hover:text-white transition-colors" href="https://www.linkedin.com/in/dennychen/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-external-link w-5 h-5"><path d="M15 3h6v6"></path><path d="M10 14 21 3"></path><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path></svg></a></div></div></div></footer></div></div><script src="./_next/static/chunks/webpack-d7ecf9c8ecc55f45.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3704,[\"944\",\"static/chunks/944-9b7549ec5596ba95.js\",\"177\",\"static/chunks/app/layout-5db32462581884c2.js\"],\"\"]\n3:I[889,[\"944\",\"static/chunks/944-9b7549ec5596ba95.js\",\"177\",\"static/chunks/app/layout-5db32462581884c2.js\"],\"default\"]\n4:I[5244,[],\"\"]\n5:I[3866,[],\"\"]\n6:I[7033,[],\"ClientPageRoot\"]\n7:I[2136,[\"944\",\"static/chunks/944-9b7549ec5596ba95.js\",\"392\",\"static/chunks/392-ced4cbe0c5868303.js\",\"113\",\"static/chunks/113-8d32fc47c3af7cd3.js\",\"214\",\"static/chunks/app/datasets/page-c2705cce1ea496d1.js\"],\"default\"]\na:I[6213,[],\"OutletBoundary\"]\nc:I[6213,[],\"MetadataBoundary\"]\ne:I[6213,[],\"ViewportBoundary\"]\n10:I[4835,[],\"\"]\n:HL[\"./_next/static/css/875eb13cb899d14e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"vX4UyZ706tQ1WTGB_OpMM\",\"p\":\".\",\"c\":[\"\",\"datasets\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"datasets\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"./_next/static/css/875eb13cb899d14e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.googleapis.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.gstatic.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"href\":\"https://fonts.googleapis.com/css2?family=Geist\u0026family=Geist+Mono\u0026display=swap\",\"rel\":\"stylesheet\"}],[\"$\",\"$L2\",null,{\"crossOrigin\":\"anonymous\",\"src\":\"//unpkg.com/same-runtime/dist/index.global.js\"}]]}],[\"$\",\"body\",null,{\"suppressHydrationWarning\":true,\"className\":\"antialiased\",\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[\"datasets\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"datasets\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L6\",null,{\"Component\":\"$7\",\"searchParams\":{},\"params\":{},\"promises\":[\"$@8\",\"$@9\"]}],null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"GHvERwZk1qFQVrvlxuj9P\",{\"children\":[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],null]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"8:{}\n9:{}\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nd:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Dr. Chen Chen\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Generated by create next app\"}]]\n"])</script><script>self.__next_f.push([1,"b:null\n"])</script></body></html>