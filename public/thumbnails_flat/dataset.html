<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Dr. Chen Chen </title>
</head>
<body>

  <script type="text/javascript"> function showemail(d,u)

      {

      var email = u + "@" + d;

      document.write("<a href=mailto:"+email+">"+email+"</a>");

      }
    </script>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Dr. Chen</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="biography.html">Biography</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="research.html">Research</a>
<div class="menu-item"><a href="publication.html">Publication</a></div>
<div class="menu-item"><a href="patent.html">Patent</a>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="activity.html">Activities</a></div>
<div class="menu-item"><a href="dataset.html">Dataset</a></div>
<div class="menu-item"><a href="join.html">Join</a></div>


</td>
<td id="layout-content">
<div id="toptitle">
<h1>Dataset</h1>
<!--
<div id="subtitle">
<a href="https://ece.uncc.edu/">Department of Electrical Engineering</a>,
<a href="https://www.uncc.edu/">University of North Carolina at Charlotte</a></div>
-->
</div>






All data is only for research purposes, unless stated differently. Please make sure to reference the authors properly when using the data.
<br>
<br>

<h2><b> Video Anomaly Dection Dataset</b></h2>

<table class="imgtable"><tr><td>
<img src="img/thumbnail/ucf-crime.png" alt="" width="350px" height="300px" />&nbsp;</td>
<!--<td>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp</td>-->

<td align="left"> UCF-Crime dataset is a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed 
real-world surveillance videos, with 13 realistic anomalies including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, 
Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism. These anomalies are selected because they have a significant impact on public safety.
This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. 
Second, for recognizing each of 13 anomalous activities. 
<br><br>
<b>Real-world Anomaly Detection in Surveillance Videos</b> <br />
Waqas Sultani, 
Chen Chen,
Mubarak Shah <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018</i> <br />
[<a href="https://arxiv.org/pdf/1801.04264.pdf">Paper</a>] 
[<a href="http://crcv.ucf.edu/projects/real-world/">Project Website</a>]
[</font><a href="ReadMe-Anomaly-Detection.txt">Note</a>]
[</font><a href="https://github.com/WaqasSultani/AnomalyDetectionCVPR2018">Code</a>]
[<a href="https://visionlab.uncc.edu/download/summary/60-data/477-ucf-anomaly-detection-dataset"><font COLOR="#FF0000">Download the dataset</font></a>] <br>
(Note: The "Anomaly_Train.txt" file in the zip file is corrupted, please down it here:  <a href="Anomaly_Train.txt">Anomaly_Train.txt</a>) <br>
<u><font COLOR="#FF0000">Option 2: Download the dataset from Dropbox (multiple files)</font>: <a href="https://www.dropbox.com/sh/75v5ehq4cdg5g5g/AABvnJSwZI7zXb8_myBA0CLHa?dl=0">Link</a></u>  <br>

</td></tr></table>



<h2><b> Satellite Smoke Scene Detection Dataset   </b></h2>

<table class="imgtable"><tr><td>
<img src="img/thumbnail/Smokenet_dataset.png" alt="" width="350px" height="300px" />&nbsp;</td>

<td align="left"> One important challenge for detecting fire smoke in satellite imagery is the similar disasters and multiple land covers. 
The commonly used smoke detection methods mainly focus on smoke discrimination from a few specific classes, which reduces 
their applicability in different regions of various classes. In addition, there is no satellite remote sensing smoke detection dataset so far.
To this end, we construct the USTC_SmokeRS dataset and integrate more smoke-like aerosol classes and land covers in the dataset, 
for example, cloud, dust, haze, bright surfaces, lakes, seaside, vegetation, etc. The USTC_SmokeRS dataset contains a total of 6225 RGB images from six classes: 
cloud, dust, haze, land, seaside, and smoke. Each image was saved as the ".tif" format with the size of 256 Ã— 256. 
<br><br>
<b>SmokeNet: Satellite Smoke Scene Detection Using Convolutional Neural Network with Spatial and Channel-Wise Attention</b> <br />
Rui Ba,
Chen Chen,
Jiang Yuan,
Weiguo Song,
Siuming Lo<br />
<i>Remote Sensing, 2019.</i> <br />
[<a href="smokenet.pdf">Paper</a>]  [<a href="http://complex.ustc.edu.cn/2019/0802/c18202a389656/page.htm">Project Website</a>]  
[<a href="https://drive.google.com/file/d/12lca2xE1LvZaJF73EFBdN1MzFw3jF5ev/view?usp=sharing">Download Dataset from Google Drive</a>]
[<a href="https://onedrive.live.com/?authkey=%21AFYQkl1tP%2DQh3Ek&id=2B888FC2F8F47809%21857&cid=2B888FC2F8F47809">Download Dataset from OneDrive</a>]
[<a href="https://pan.baidu.com/s/1GBOE6xRVzEBV92TrRMtfWg">Download Dataset from Baidu Pan (download password: 5dlk)</a>]
</td></tr></table>



<h2><b> Cross-View Geolocalization Dataset   </b></h2>

<table class="imgtable"><tr><td>
<img src="img/thumbnail/geo.png" alt="" width="350px" height="300px" /> &nbsp;</td>

<td align="left"> UCF cross-view geolocalization dataset is created for the geo-localization task using cross-view image matching.
The dataset has street view and bird's eye view image pairs around downtown Pittsburg, 
Orlando and part of Manhattan. 
There are 1,586, 1,324 and 5,941 GPS locations in Pittsburg, Orlando and Manhattan, respectively. 
We utilize DualMaps to generate side-by-side street view and bird's eye view images at each GPS location with the same heading direction. 
The street view images are from Google and the overhead 45 degree bird's eye view images are from Bing. 
For each GPS location, four image pairs are generated with camera heading directions of 0 degree, 90 degree, 180 degree and 270 degree. 
In order to learn the deep network for building matching, we annotate corresponding buildings in every street view and bird's eye view image pair.
<br><br>

<b>Cross-View Image Matching for Geo-localization in Urban Environments</b> <br />
Yicong Tian, Chen Chen, Mubarak Shah <br />
<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017</i> <br />
[<a href="https://arxiv.org/pdf/1703.07815.pdf">Paper</a>]  
[<a href="http://crcv.ucf.edu/data/Cross-View/">Project (<font COLOR="#FF0000">Download Cross-view dataset and code</font>)</a>]
</td></tr></table>



<h2><b> UTD-MHAD Dataset   </b></h2>

<table class="imgtable"><tr><td>
<img src="img/thumbnail/utd.png" alt="" width="350px" height="350px" />&nbsp;</td>

<td align="left"> UTD-MHAD dataset was collected as part of our research on human action recognition using fusion of depth and inertial sensor data. 
The objective of this research has been to develop algorithms for more robust human action recognition using fusion of data from differing modality sensors.
The UTD-MHAD dataset consists of 27 different actions: (1) right arm swipe to the left, (2) right arm swipe to the right, (3) right hand wave, (4) two hand front clap, (5) right arm throw, (6) cross arms in the chest, (7) basketball shoot, (8) right hand draw x, (9) right hand draw circle (clockwise), (10) right hand draw circle (counter clockwise), (11) draw triangle, (12) bowling (right hand), (13) front boxing, (14) baseball swing from right, (15) tennis right hand forehand swing, (16) arm curl (two arms), (17) tennis serve, (18) two hand push, (19) right hand knock on door, 
(20) right hand catch an object, (21) right hand pick up and throw, (22) jogging in place, (23) walking in place, (24) sit to stand, (25) stand to sit, (26) forward lunge (left foot forward), (27) squat (two arms stretch out).
<br><br>
<b>UTD-MHAD: A Multimodal Dataset for Human Action Recognition Utilizing a Depth Camera and a Wearable Inertial Sensor</b> <br />
Chen Chen, Roozbeh Jafari, Nasser Kehtarnavaz <br />
<i> IEEE International Conference on Image Processing (ICIP), 2015</i> <br />
[<a href="ICIP2015-Chen-Final.pdf">Paper</a>] 
[<a href="http://www.utdallas.edu/~kehtar/UTD-MHAD.html">UTD Multimodal Human Action Dataset Website</a>] 
</td></tr></table>



<br>
<br>
</td>
</tr>
</table>
</body>
</html>





